{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis (draft)\n",
    "\n",
    "This notebook contains the Python script that analyses the data extracted from the STRESS review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "# use gglot style for all matplotlib\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW_CSV_FILE_PATH = \"PY_STRESS.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility functions\n",
    "\n",
    "Some simple functions repeatedly used for plotting or analysing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def frequency_bar_chart(\n",
    "    data: pd.DataFrame, \n",
    "    x_label: str,\n",
    "    y_label: Optional[str] = \"Frequency\",\n",
    "    rotate_x_ticks: Optional[int] = 0,\n",
    "    figsize: Optional[int|int] = (12,6)\n",
    "):\n",
    "    '''\n",
    "    Create bar chart of the selected categorical variable.\n",
    "    Returns matplotlib figure and axis.\n",
    "\n",
    "    Parameters:\n",
    "    ---------\n",
    "    data: pd.DataFrame\n",
    "       Frequency dataset for bar chart\n",
    "\n",
    "    x_label: str\n",
    "        Label to display on x-axis\n",
    "\n",
    "    y_label: str, optional (default = \"Frequency\")\n",
    "        Label to display on y-axis\n",
    "\n",
    "    rotate_x_ticks: int, optional (default = 0)\n",
    "        Degress to rotate the x axis text (0 for no rotation)\n",
    "\n",
    "    figsize: Tuple(int, int), optional (default=(12,6)\n",
    "        The size of the matplotlib picture.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    out: fig and axis of plot\n",
    "    '''\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    ax = data.plot(kind=\"bar\", ax=ax)\n",
    "\n",
    "    _ = ax.set_xlabel(x_label)\n",
    "    _ = ax.set_ylabel(y_label)\n",
    "    \n",
    "    # Add data labels on the bars\n",
    "    _ = ax.bar_label(ax.containers[0], label_type=\"edge\", padding=3)\n",
    "\n",
    "    # rotate x axis text\n",
    "    _ = plt.xticks(rotation=rotate_x_ticks)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read in review data\n",
    "\n",
    "> Note this data collected in Excel.\n",
    "\n",
    "::: {.callout-caution collapse=\"true\"}\n",
    "## TM Queries about data\n",
    "\n",
    "* **TM query (1)**: can we drop column index 27? This is labelled 27 and is all values are null.  I've removed it in the code below...\n",
    "\n",
    "* **TM query (2)**: there were three blank lines at the end of the CSV. I've removed this in the updated load routine.\n",
    "\n",
    "* **TM query (3)**: what does DP mean in \"used?\"\n",
    "\n",
    "* **TM query (4)**: target authors: is this a manual field you have included? or did you have a formula in Excel? It appears to be different from your calculation at the end?\n",
    "\n",
    "* **TM query (5)**: To discuss with others -> I think we should just exclude all studies not in English.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data cleaning functions\n",
    "\n",
    "Smaller function used by the main pipeline to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def recode_whitespace(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    recode whitespace as \"_\" and strip head/tail whitespace just in case\n",
    "    '''\n",
    "    # strip leading and lagging white space\n",
    "    df.columns = df.columns.str.strip()\n",
    "    # replace remaining whitespace with \"_\"\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def strip_punctuation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    strip select punction from column headers\n",
    "    '''\n",
    "    df.columns = df.columns.str.replace(\"?\", \"\")\n",
    "    df.columns = df.columns.str.replace(\"'\", \"\")\n",
    "    df.columns = df.columns.str.replace(\"-\", \"_\")\n",
    "    df.columns = df.columns.str.replace(\"/\", \"\")\n",
    "    return df\n",
    "\n",
    "def cols_to_lower(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert all column names in a dataframe to lower case\n",
    "\n",
    "    Params:\n",
    "    ------\n",
    "    df - pandas.DataFrame\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    out: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    new_cols = [c.lower() for c in df.columns]\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def drop_non_english_language(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Check for \"Not in English\" recorded in any column\n",
    "    Return all other rows.\n",
    "    '''\n",
    "    return df[~df.isin(['Not in English']).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load Data Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def load_review_dataset(\n",
    "    path: Optional[str] = REVIEW_CSV_FILE_PATH,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read full data extraction data set for the review from a CSV file.\n",
    "    Returns cleaned dataset.\n",
    "\n",
    "    Assumes data is stored in .csv\n",
    "\n",
    "    Cleaning pipeline for dataset:\n",
    "    1. drop redunance columns\n",
    "    2. drop rows that contain all NAs\n",
    "    3. drop rows that are no in English\n",
    "    3. rename columns with complex strings\n",
    "    4. strip all punctuation from column headers\n",
    "    5. column headers to lower case\n",
    "    6. replace all whitespace in headers with \"_\"\n",
    "    7. convert all blank strings in cells to NaN\n",
    "    8. recode variables to be internally consistent in naming\n",
    "    9. perform type conversion for integer fields\n",
    "    10. type conversion for categorical fields\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    path: str, optional (default=REVIEW_CSV_FILE_PATH)\n",
    "        path or URL for review dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    out: pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # SETUP FOR DATASET CLEAN\n",
    "\n",
    "    # cols to drop from data read in\n",
    "    cols_to_remove = [\n",
    "        \"Unnamed: 0\",\n",
    "        \"...27\",\n",
    "        \"Questions to be asked from authors / experts\",\n",
    "        \"Fatemeh's Note\",\n",
    "        \"Note\",\n",
    "    ]\n",
    "\n",
    "    # simple type conversions\n",
    "    type_conversions = {\"year\": \"UInt16\"}\n",
    "\n",
    "    # renaming of columns to names suitable for analysis\n",
    "    new_labels = {\n",
    "        \"1. Objectives (purpose, model outputs, aims of experimentation)\": \"stress_objectives\",\n",
    "        \"2. Logic (base model overview diagram, base model logic, scenario logic, algorithms, components)\": \"stress_logic\",\n",
    "        \"3. Data (data sources, input parameters, preprocessing, assumptions\": \"stress_logic\",\n",
    "        \"4. Experimentation (initialisation, run length, estimation approach)\": \"stress_exp\",\n",
    "        \"5. Implementation (software and programming language, random sampling, model execution, system specification)\": \"stress_imp\",\n",
    "        \"6. Code access (computer model sharing statement)\": \"stress_code\",\n",
    "    }\n",
    "\n",
    "    # used to recode variables so they are consistent.\n",
    "    recoded_variables = {\"used\": {\"NO\": \"No\"}}\n",
    "\n",
    "    # DATA CLEANING PIPELINE\n",
    "    clean = (\n",
    "        pd.read_csv(path, index_col=\"No\")\n",
    "        # drop redundant index column\n",
    "        .drop(labels=cols_to_remove, axis=1)\n",
    "        # drop all blank rows (this will remove the blank 3 rows at end of CSV)\n",
    "        .dropna(how=\"all\")\n",
    "        # drop all studies that do not use an English language\n",
    "        .pipe(drop_non_english_language)\n",
    "        # rename verbose column headers\n",
    "        .rename(columns=new_labels)\n",
    "        # remove any punctutation i.e. \"?\" and \"-\"\n",
    "        .pipe(strip_punctuation)\n",
    "        # all columns headers to lower case\n",
    "        .pipe(cols_to_lower)\n",
    "        # replace all whitespace with \"_\" in col headers\n",
    "        .pipe(recode_whitespace)\n",
    "        # replace all whitespace and blank strings in fields with NaN\n",
    "        .replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "        # recoded variables e.g. used \"NO\" becomes \"No\"\n",
    "        .replace(recoded_variables)\n",
    "        # update the type of columns to int where needed\n",
    "        .astype(type_conversions)\n",
    "        # categorical variables\n",
    "        .assign(\n",
    "            used=lambda x: pd.Categorical(x[\"used\"]),\n",
    "            type_of_paper=lambda x: pd.Categorical(x[\"type_of_paper\"]),\n",
    "            partially=lambda x: pd.Categorical(x[\"partially\"]),\n",
    "            method=lambda x: pd.Categorical(x[\"method\"]),\n",
    "            software=lambda x: pd.Categorical(x[\"software\"]),\n",
    "            source_code_access=lambda x: pd.Categorical(\n",
    "                x[\"source_code_access\"]\n",
    "            ),\n",
    "            application_area=lambda x: pd.Categorical(x[\"application_area\"]),\n",
    "            target_authors=lambda x: pd.Categorical(x[\"target_authors\"]),\n",
    "            stress_implementation=lambda x: pd.Categorical(\n",
    "                x[\"stress_implementation\"]\n",
    "            ),\n",
    "            hybridisation=lambda x: pd.Categorical(x[\"hybridisation\"])\n",
    "        )\n",
    "    )\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 165 entries, 1 to 171\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   publication            165 non-null    object  \n",
      " 1   authors                165 non-null    object  \n",
      " 2   year                   163 non-null    UInt16  \n",
      " 3   type_of_paper          163 non-null    category\n",
      " 4   journal                161 non-null    object  \n",
      " 5   name_of_univerity      73 non-null     object  \n",
      " 6   type_of_study          164 non-null    object  \n",
      " 7   pre_prints             165 non-null    object  \n",
      " 8   doi                    159 non-null    object  \n",
      " 9   used                   161 non-null    category\n",
      " 10  partially              71 non-null     category\n",
      " 11  target_authors         73 non-null     category\n",
      " 12  method                 73 non-null     category\n",
      " 13  hybridisation          7 non-null      category\n",
      " 14  stress_objectives      70 non-null     object  \n",
      " 15  stress_logic           70 non-null     object  \n",
      " 16  stress_logic           70 non-null     object  \n",
      " 17  stress_exp             70 non-null     object  \n",
      " 18  stress_imp             70 non-null     object  \n",
      " 19  stress_code            70 non-null     object  \n",
      " 20  source_code_access     40 non-null     category\n",
      " 21  software               69 non-null     category\n",
      " 22  application_area       74 non-null     category\n",
      " 23  case_study             72 non-null     object  \n",
      " 24  stress_implementation  70 non-null     category\n",
      "dtypes: UInt16(1), category(10), object(14)"
     ]
    }
   ],
   "source": [
    "# read in data.\n",
    "clean = load_review_dataset()\n",
    "\n",
    "# quick summary of columns\n",
    "clean.info(memory_usage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_review_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# rows and columns n's\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclean_review_df\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_review_df' is not defined"
     ]
    }
   ],
   "source": [
    "# rows and columns n's\n",
    "clean_review_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter to empirical studies only \n",
    "\n",
    "Here we separate the studies that have used the STRESS guidelines in second dataframe i.e. limit to studies that have used STRESS for documenting a model. This is stored in a notebook level variable called `USED_STRESS`\n",
    "\n",
    "> Filtering is done using the 'used' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_application_studies(clean_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter the cleaned dataset down to studies that used stress to report\n",
    "    a simulation study.\n",
    "\n",
    "    # To do: drop \"fatemets_notes\", \"questions...\"\n",
    "    \"\"\"\n",
    "    # Used?: a Yes/No variable.\n",
    "    filtered_df = clean_df[clean_df[\"used\"] == \"Yes\"]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_STRESS = filter_to_application_studies(clean_review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of studies that used STRESS as intended i.e. to document\n",
    "USED_STRESS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "### Year of publication\n",
    "\n",
    "\n",
    "::: {.callout-note collapse=\"true\"}\n",
    "## TM notes for additional analysis\n",
    "\n",
    "* **TM Query (1)**: 2024 obviously partial, as it will take us a while to do this study we should update again in 2025 to get all 2024 papers if we can\n",
    "\n",
    "*  **TM Query (2)**: WE should prob show no. citations OVERALL by year as well\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_1(data: pd.DataFrame, figsize: Optional[int|int] = (12,6)):\n",
    "    '''\n",
    "    Create bar chart of publications by year (ordered) \n",
    "\n",
    "    Parameters:\n",
    "    ---------\n",
    "    data: pd.DataFrame\n",
    "        The cleaned + filtered review data to plot\n",
    "\n",
    "    figsize = Tuple(int, int)\n",
    "        Size of matplotlib figure\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    out: fig and axis of plot\n",
    "    '''\n",
    "\n",
    "    # The frequency of papers that have used STRESS guidelines \n",
    "    # based on the publication year\n",
    "    year_freq = data[\"year\"].value_counts().sort_index(ascending=True)\n",
    "\n",
    "    return frequency_bar_chart(data=year_freq, \n",
    "                               x_label=\"Year\", \n",
    "                               y_label=\"Publications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figure_1(USED_STRESS, False)\n",
    "fig.savefig('./figures/figure_1.png', dpi=300, bbox_inches=\"tight\")\n",
    "_ = fig.suptitle(\"Fig 1. Frequency of Empirical Papers over Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation method \n",
    "\n",
    "::: {.callout-note collapse=\"true\"}\n",
    "## Data notes\n",
    "\n",
    "Hybrid M&S could be the hybridisation of simulation methods (e.g. SD+DES) or hybridisation of a simulation method with data science approach (e.g. Monte Carlo simulation + Machine learning)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_2(data: pd.DataFrame, figsize: Optional[int|int] = (12,6)):\n",
    "    '''\n",
    "    Create bar chart of simulation method used in studies \n",
    "\n",
    "    Parameters:\n",
    "    ---------\n",
    "    data: pd.DataFrame\n",
    "        The cleaned + filtered review data to plot\n",
    "\n",
    "    figsize = Tuple(int, int)\n",
    "        Size of matplotlib figure\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    out: fig and axis of plot\n",
    "    '''\n",
    "\n",
    "    method_freq = data[\"method\"].value_counts()\n",
    "\n",
    "    return frequency_bar_chart(data=method_freq, \n",
    "                               x_label=\"Method\", \n",
    "                               y_label=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figure_2(USED_STRESS, False)\n",
    "fig.savefig('./figures/figure_2.png', dpi=300, bbox_inches=\"tight\")\n",
    "_ = fig.suptitle(\"Fig 2. Frequency of Simulation Methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_3(data: pd.DataFrame, figsize: Optional[int|int] = (12,6)):\n",
    "    '''Frequency of software/coding language usage as bar chart\n",
    "\n",
    "    Parameters:\n",
    "    ---------\n",
    "    data: pd.DataFrame\n",
    "        The cleaned + filtered review data to plot\n",
    "\n",
    "    figsize = Tuple(int, int)\n",
    "        Size of matplotlib figure\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    out: fig and axis of plot\n",
    "    '''\n",
    "\n",
    "    freq = data[\"application_area\"].value_counts()\n",
    "\n",
    "    return frequency_bar_chart(data=freq, \n",
    "                               x_label=\"Application area\", \n",
    "                               y_label=\"Frequency\", \n",
    "                               rotate_x_ticks=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figure_3(USED_STRESS, False)\n",
    "fig.savefig('./figures/figure_3.png', dpi=300, bbox_inches=\"tight\")\n",
    "_ = fig.suptitle(\"Fig 3. Application area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_4(data: pd.DataFrame, figsize: Optional[int|int] = (12,6)):\n",
    "    '''Frequency of software/coding language usage as bar chart\n",
    "\n",
    "    Parameters:\n",
    "    ---------\n",
    "    data: pd.DataFrame\n",
    "        The cleaned + filtered review data to plot\n",
    "\n",
    "    figsize = Tuple(int, int)\n",
    "        Size of matplotlib figure\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    out: fig and axis of plot\n",
    "    '''\n",
    "\n",
    "    freq = data[\"type_of_paper\"].value_counts()\n",
    "\n",
    "    return frequency_bar_chart(data=freq, \n",
    "                               x_label=\"Application area\", \n",
    "                               y_label=\"Frequency\", \n",
    "                               rotate_x_ticks=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figure_4(USED_STRESS, False)\n",
    "fig.savefig('./figures/figure_4.png', dpi=300, bbox_inches=\"tight\")\n",
    "_ = fig.suptitle(\"Fig 4. Breakdown of the literature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How was STRESS used?\n",
    "\n",
    "The guideline have been listed either in main text, or in appendix. Also it might be in the form of checklist, structured (that contains the guidelines' elements with  description), and unstructured (which might contain some elements of the guideline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_4(data: pd.DataFrame, figsize: Optional[int|int] = (12,6)):\n",
    "    '''Frequency of type of paper\n",
    "\n",
    "    Type of papers that have used STRESS; which includes journal, conference,\n",
    "    workshop, and preprints papers as well as thesis.\n",
    "\n",
    "    Parameters:\n",
    "    ---------\n",
    "    data: pd.DataFrame\n",
    "        The cleaned + filtered review data to plot\n",
    "\n",
    "    figsize = Tuple(int, int)\n",
    "        Size of matplotlib figure\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    out: fig and axis of plot\n",
    "    '''\n",
    "\n",
    "    freq = data[\"stress_implementation\"].value_counts()\n",
    "\n",
    "    return frequency_bar_chart(data=freq, \n",
    "                               x_label=\"Approach\", \n",
    "                               y_label=\"Frequency\", \n",
    "                               rotate_x_ticks=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figure_4(USED_STRESS, False)\n",
    "fig.savefig('./figures/figure_4.png', dpi=300, bbox_inches=\"tight\")\n",
    "_ = fig.suptitle(\"Fig 4. Frequency of Papers Based on How They Implemented STRESS Guidelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage by Journal/conference/other\n",
    "\n",
    "> **TM query**: Computational Management Science appears twice? small different in raw data recording?\n",
    "\n",
    ">  **TM query**: There are 3 univrsity of southampton and 2 Lancaster Uni publications. I think we need to double check that we are not double counting with academic journals or conference papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Journal_freq = USED_STRESS[\"journal\"].value_counts()\n",
    "\n",
    "Journal_freq = Journal_freq.reset_index()\n",
    "Journal_freq.columns = [\"Journal\", \"Frequency\"]\n",
    "\n",
    "Journal_freq.set_index([\"Journal\"], inplace=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "Journal_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full versus partial usage of STRESS sections\n",
    "\n",
    "> TM note - we should provide %'s as well as counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Partially_freq = USED_STRESS[\"partially\"].value_counts()\n",
    "\n",
    "Partially_freq = Partially_freq.reset_index()\n",
    "Partially_freq.columns = [\"Partially?\", \"Frequency\"]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "Partially_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the coded model available?\n",
    "\n",
    "> TM note: cannot be shared for confidential reasons. I think this should be recoded to = \"No\"\n",
    "\n",
    "> TM note: will be available in the future should be recoded as \"No\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code_Access_freq = USED_STRESS[\"source_code_access\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "SCAB = Code_Access_freq.plot(kind=\"bar\")\n",
    "\n",
    "plt.title(\"Fig 7. Frequency of Papers based on Type of Source-Code Access\")\n",
    "plt.xlabel(\"Type of Access\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Add data labels on the bars\n",
    "SCAB.bar_label(SCAB.containers[0], label_type=\"edge\", padding=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many of the publications contain at least one author from STRESS 1.0?\n",
    "\n",
    "> TM note - the no target author list seems wrong!\n",
    ">\n",
    "> Something note right?  Should we just be using the manual target field from the dataset that gives the same answer as in the summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_authors = [\"Monks\", \"Currie\", \"Onggo\", \"Robinson\", \"Kunc\", \"Taylor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_publications_by_stress_authors(target_authors):\n",
    "#     \"\"\"of\n",
    "#     Returns two dataframes.  One contains all publications where at least one\n",
    "#     of the authors was a member of the original stress team.  The second\n",
    "#     excludes all stress team members.\n",
    "#     \"\"\"\n",
    "\n",
    "#     target_included = []\n",
    "#     not_target_authors = []\n",
    "\n",
    "#     for index, row in USED_STRESS.iterrows():\n",
    "#         authors_in_row = row[\"authors\"]\n",
    "\n",
    "#         # Check for missing or NaN values\n",
    "#         if pd.isna(authors_in_row):\n",
    "#             not_target_authors.append(\n",
    "#                 None\n",
    "#             )  # If it's NaN, consider as not_target_authors\n",
    "#             continue\n",
    "\n",
    "#         authors_in_row = authors_in_row.split(\n",
    "#             \", \"\n",
    "#         )  # Assuming author names are separated by ', '\n",
    "\n",
    "#         for authors in authors_in_row:\n",
    "#             if any(\n",
    "#                 target_authors in authors for target_authors in target_authors\n",
    "#             ):\n",
    "#                 target_included.append(\n",
    "#                     row\n",
    "#                 )  # Append the entire row's authors to the target_included list\n",
    "\n",
    "#             else:\n",
    "#                 not_target_authors.append(\n",
    "#                     row\n",
    "#                 )  # Append the entire row's authors to the not_target_authors list\n",
    "\n",
    "#     df_target_included = pd.DataFrame(target_included, columns=[\"authors\"])\n",
    "#     df_not_target_authors = pd.DataFrame(\n",
    "#         not_target_authors, columns=[\"authors\"]\n",
    "#     )\n",
    "#     return df_target_included, df_not_target_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the DataFrames\n",
    "# print(\n",
    "#     f\"Papers including at least one STRESS author: n = {len(df_target_included)}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Papers with no STRESS authors n = {len(df_not_target_authors)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
